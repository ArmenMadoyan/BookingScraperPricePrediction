{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e6af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2b1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMe(url: str,headers: str) -> BeautifulSoup:\n",
    "    response =requests.get(url,headers=headers)\n",
    "    #print(response)\n",
    "    assert response.status_code==200, \"Page didn't respond\"\n",
    "    pg = response.content\n",
    "    return BeautifulSoup(pg,'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13471ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeMe(page: BeautifulSoup) -> List[str]:\n",
    "    a = page.find('div','hotel-facilities__list').find_all('div',class_='bui-spacer--large')\n",
    "\n",
    "    types=[]\n",
    "    avails=[]\n",
    "\n",
    "    for i in a:\n",
    "        types.append(i.find('div', class_=\"bui-title__text hotel-facilities-group__title-text\").get_text().split('\\n')[1])\n",
    "        avail = i.find('ul').find_all('li')\n",
    "\n",
    "        s=\"\"\n",
    "        for j in avail:\n",
    "            if j!=avail[-1]:\n",
    "                s +=  j.find('div', class_=\"bui-list__body\").get_text().split('\\n\\n')[1] + ' | '\n",
    "            else:\n",
    "                s +=  j.find('div', class_=\"bui-list__body\").get_text().split('\\n\\n')[1]\n",
    "        avails.append(s)\n",
    "        \n",
    "    return types,avails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7590dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictMe(types: List[str], avails: List[str]) -> dict:\n",
    "    assert len(types)==len(avails), \"The lengths don't match\"\n",
    "    d=dict(zip(types,avails))\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a6ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdMe(d: dict) -> pd.DataFrame:\n",
    "    df = pd.DataFrame([d])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d328edf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     d \u001b[38;5;241m=\u001b[39m dictMe(a,b)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pdMe(d)\n\u001b[0;32m---> 11\u001b[0m df\u001b[38;5;241m=\u001b[39mscMeBooking(url,headers)\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mscMeBooking\u001b[0;34m(url, headers)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscMeBooking\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m,headers: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      5\u001b[0m     pg \u001b[38;5;241m=\u001b[39m parseMe(url, headers)\n\u001b[0;32m----> 6\u001b[0m     a,b \u001b[38;5;241m=\u001b[39m scrapeMe(pg)\n\u001b[1;32m      7\u001b[0m     d \u001b[38;5;241m=\u001b[39m dictMe(a,b)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pdMe(d)\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mscrapeMe\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrapeMe\u001b[39m(page: BeautifulSoup) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     a \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhotel-facilities__list\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbui-spacer--large\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     types\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      5\u001b[0m     avails\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'}\n",
    "url ='https://www.booking.com/hotel/am/bed-and-breakfast-dilijan-orran.html?aid=304142&label=gen173nr-1FCAEoggI46AdIM1gEaAeIAQGYATG4ARnIAQzYAQHoAQH4AQKIAgGoAgO4At-Qs5gGwAIB0gIkNjYzZDQ0Y2YtMzMyZi00YjJiLWI1ODYtOTUzOTNhMGEwYTYz2AIF4AIB&sid=4c9da70ca7c98f7e6414f416852ae238&dest_id=-2324702;dest_type=city;dist=0;group_adults=2;group_children=0;hapos=2;hpos=2;no_rooms=1;req_adults=2;req_children=0;room1=A%2CA;sb_price_type=total;sr_order=popularity;srepoch=1661782167;srpvid=1e91638b6fd3024d;type=total;ucfs=1&#hp_facilities_box'\n",
    "\n",
    "def scMeBooking(url: str,headers: str) -> pd.DataFrame:\n",
    "    pg = parseMe(url, headers)\n",
    "    a,b = scrapeMe(pg)\n",
    "    d = dictMe(a,b)\n",
    "    return pdMe(d)\n",
    "    \n",
    "\n",
    "df=scMeBooking(url,headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d2973-4e5c-48d2-8033-eb0c72bdbf89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2820cd03-7cd9-45da-b739-64f6bf8fba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1: Example Domain\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://example.com\"\n",
    "\n",
    "# Headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send the request with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract headers from the page\n",
    "    headers_data = {\n",
    "        \"h1\": [h1.text.strip() for h1 in soup.find_all(\"h1\")],\n",
    "        \"h2\": [h2.text.strip() for h2 in soup.find_all(\"h2\")],\n",
    "        \"h3\": [h3.text.strip() for h3 in soup.find_all(\"h3\")]\n",
    "    }\n",
    "    \n",
    "    # Print extracted headers\n",
    "    for tag, texts in headers_data.items():\n",
    "        for text in texts:\n",
    "            print(f\"{tag.upper()}: {text}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve page, status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d08e28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the '....' div.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def parseMe(url: str, headers: str) -> BeautifulSoup:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    assert response.status_code == 200, \"Page didn't respond\"\n",
    "    pg = response.content\n",
    "    return BeautifulSoup(pg, 'html.parser')\n",
    "\n",
    "def scrapeMe(page: BeautifulSoup) -> List[str]:\n",
    "    facilities_list = page.find('div', 'property-most-popular-facilities-wrapper')\n",
    "    \n",
    "    if not facilities_list:\n",
    "        print(\"Could not find the '....' div.\")\n",
    "        return [], []\n",
    "    \n",
    "    a = facilities_list.find_all('div', class_='bui-spacer--large')\n",
    "    \n",
    "    types = []\n",
    "    avails = []\n",
    "\n",
    "    for i in a:\n",
    "        type_element = i.find('div', class_=\"bui-title__text hotel-facilities-group__title-text\")\n",
    "        if not type_element:\n",
    "            print(\"Could not find the 'bui-title__text hotel-facilities-group__title-text' div.\")\n",
    "            continue\n",
    "        \n",
    "        type_text = type_element.get_text().split('\\n')[1]\n",
    "        types.append(type_text)\n",
    "        \n",
    "        avail = i.find('ul')\n",
    "        if not avail:\n",
    "            print(\"Could not find the 'ul' element.\")\n",
    "            continue\n",
    "        \n",
    "        avail_items = avail.find_all('li')\n",
    "        s = \"\"\n",
    "        for j in avail_items:\n",
    "            body_element = j.find('div', class_=\"bui-list__body\")\n",
    "            if not body_element:\n",
    "                print(\"Could not find the 'bui-list__body' div.\")\n",
    "                continue\n",
    "            \n",
    "            s += body_element.get_text().split('\\n\\n')[1] + ' | ' if j != avail_items[-1] else body_element.get_text().split('\\n\\n')[1]\n",
    "        \n",
    "        avails.append(s)\n",
    "        \n",
    "    return types, avails\n",
    "\n",
    "def dictMe(types: List[str], avails: List[str]) -> dict:\n",
    "    assert len(types) == len(avails), \"The lengths don't match\"\n",
    "    d = dict(zip(types, avails))\n",
    "    return d\n",
    "\n",
    "def pdMe(d: dict) -> pd.DataFrame:\n",
    "    df = pd.DataFrame([d])\n",
    "    return df\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'}\n",
    "url = 'https://www.booking.com/hotel/am/bed-and-breakfast-dilijan-orran.html?aid=304142&label=gen173nr-1FCAEoggI46AdIM1gEaAeIAQGYATG4ARnIAQzYAQHoAQH4AQKIAgGoAgO4At-Qs5gGwAIB0gIkNjYzZDQ0Y2YtMzMyZi00YjJiLWI1ODYtOTUzOTNhMGEwYTYz2AIF4AIB&sid=4c9da70ca7c98f7e6414f416852ae238&dest_id=-2324702;dest_type=city;dist=0;group_adults=2;group_children=0;hapos=2;hpos=2;no_rooms=1;req_adults=2;req_children=0;room1=A%2CA;sb_price_type=total;sr_order=popularity;srepoch=1661782167;srpvid=1e91638b6fd3024d;type=total;ucfs=1&#hp_facilities_box'\n",
    "\n",
    "def scMeBooking(url: str, headers: str) -> pd.DataFrame:\n",
    "    pg = parseMe(url, headers)\n",
    "    a, b = scrapeMe(pg)\n",
    "    d = dictMe(a, b)\n",
    "    return pdMe(d)\n",
    "\n",
    "df = scMeBooking(url, headers)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
